from tokenizer import Tokenizer
from tokenizer_definition_util import regex, word, wordlist

tokenizer = Tokenizer()

tokenizer.define(word("if", "if"))
tokenizer.define(word("elif", "elif"))
tokenizer.define(word("else", "else"))
tokenizer.define(word("for", "for"))
tokenizer.define(word("in", "in"))
tokenizer.define(word("break", "break"))
tokenizer.define(wordlist("boolean", ["true", "false"]))
tokenizer.define(regex("word", r"^[a-zA-Z_$]+"))
tokenizer.define(regex("int", r"^[0-9]+"))
tokenizer.define(regex("float", r"^[0-9]+\.[0-9]*"))
tokenizer.define(word("assign", "="))
tokenizer.define(word("write", ":="))
tokenizer.define(word("pipe", "::"))
tokenizer.define(word("map", ":>"))
tokenizer.define(word("read", "->"))
tokenizer.define(word("pop", ":+"))
tokenizer.define(word("comma", ","))
tokenizer.define(word("period", "."))
tokenizer.define(word("arrow", "=>"))
tokenizer.define(word("index", ":"))
tokenizer.define(word("curly_open", "{"))
tokenizer.define(word("curly_close", "}"))
tokenizer.define(word("bracket_open", "("))
tokenizer.define(word("bracket_close", ")"))
tokenizer.define(word("square_open", "["))
tokenizer.define(word("square_close", "]"))
tokenizer.define(wordlist("binary_operator", ["+", "-", "*", "/", "??", ">", "<", ">=", "<=", "!=", "=="]))
tokenizer.define(regex("whitespace", r"^[ \t\r]+", throwaway=True))
tokenizer.define(wordlist("whitebreak", ["\n", ";"]))
tokenizer.define(regex("string", r"^\"(?:[^\"\\]|\\.)*\""))
tokenizer.define(regex("string", r"^'(?:[^'\\]|\\.)*'"))
